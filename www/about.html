<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>About â€” LLM Observatory</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>
<body class="text-light" style="background-color: #0d1b2a !important;">
  <div id="navbar"></div>
  <script>
    fetch("navbar.html")
      .then(res => res.text())
      .then(data => document.getElementById("navbar").innerHTML = data);
  </script>

  <section class="container my-5">
    <h1 class="mb-4">About the LLM Observatory</h1>

    <div class="mb-5">
      <h3>What We Study</h3>
      <p>
        The LLM Observatory is an open-source community initiative focused on evaluating how large language models perform on inference tasks grounded in observational data. We are especially interested in descriptive statistics, causal effects, and counterfactual reasoning.
      </p>
    </div>

    <div class="mb-5">
      <h3>How We Evaluate Language Models</h3>
      <p>
        We design structured benchmarks using tabular datasets and task definitions spanning multiple reasoning types (factual, interventional, counterfactual). Our evaluation metrics reflect both prediction accuracy and alignment with empirical distributions.
      </p>
    </div>

    <div class="mb-5">
      <h3>Our Data Sources</h3>
      <p>
        Our benchmarks are built on trusted U.S. population datasets such as ACS, NHANES, NSDUH, MEPS, and others. These datasets offer rich statistical structure and support robust evaluation across diverse domains.
      </p>
    </div>
  </section>
</body>
</html>
